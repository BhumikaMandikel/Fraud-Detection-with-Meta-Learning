{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Members:\n",
        "Bhumika S Mandikel              - PES1UG22AM043\n",
        "\n",
        "Arnitha Sathish                 - PES1UG22AM030\n",
        "\n",
        "Athmica Kishore K               - PES1UG22AM036\n",
        "\n",
        "Anvitha Anand                   - PES1UG22AM029"
      ],
      "metadata": {
        "id": "oBIdM-K1q6OW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reptile Meta-Learning Algorithm for financial fraud detection\n",
        "\n",
        "This notebook implements the Reptile meta-learning algorithm to train a model that adapts quickly to new binary classification tasks.We are doing this for financial fraud detection. It includes:\n",
        "- Dataset preprocessing and handling class imbalance.\n",
        "- Meta-learning using the Reptile algorithm.\n",
        "- Evaluation of the meta-learned model on a new task.\n",
        "\n",
        "The key idea is to train a meta-model that learns an initialization capable of fast adaptation to new tasks.\n"
      ],
      "metadata": {
        "id": "QWyJnHulhMlm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installing the required libraries"
      ],
      "metadata": {
        "id": "ljnwrwq8XfeR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6H95OaebByrO",
        "outputId": "b5b31465-fa71-4e84-9a93-4a91cffd65a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.25.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.67.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.1)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.5.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install numpy pandas scikit-learn tensorflow\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing Libraries\n",
        "\n",
        "We import essential libraries for:\n",
        "- Data manipulation (`numpy`, `pandas`).\n",
        "- Model building (`tensorflow`).\n",
        "- Data preprocessing and evaluation (`sklearn`).\n"
      ],
      "metadata": {
        "id": "_LiLEYfiYLfP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.utils import resample\n",
        "import time"
      ],
      "metadata": {
        "id": "g2XA515QYPOQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading and Preprocessing Data\n",
        "\n",
        "The `load_data` function:\n",
        "1. Loads a dataset from a given path.\n",
        "2. Handles missing values by filling them with zeros.\n",
        "3. Encodes categorical target variables into numeric form if necessary.\n",
        "4. Addresses class imbalance by oversampling the minority class with a controlled ratio.\n",
        "\n",
        "Oversampling ensures that the model doesn't bias towards the majority class.\n",
        "The size of the datasets chosen is arround 800 for training purposes.\n",
        "This shows how meta-learning is particularly helpful when u have a smaller amount of data.\n"
      ],
      "metadata": {
        "id": "a-ZQ1-m_bMAE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to load and preprocess datasets\n",
        "def load_data(path, oversample_ratio=0.5):\n",
        "    # Load your dataset\n",
        "    data = pd.read_csv(path)\n",
        "    data = data[:800]  # Subset data for faster experimentation\n",
        "\n",
        "    # Handle NaN values\n",
        "    if data.isnull().values.any():\n",
        "        print(f\"Warning: NaN values found in {path}. Filling with zeros.\")\n",
        "        data = data.fillna(0)\n",
        "\n",
        "    # Split into features (X) and target (y)\n",
        "    X = data.iloc[:, :-1].astype(np.float32).values\n",
        "    y = data.iloc[:, -1]\n",
        "\n",
        "    # Encode target variable if categorical\n",
        "    if y.dtype == 'object':\n",
        "        le = LabelEncoder()\n",
        "        y = le.fit_transform(y)\n",
        "    else:\n",
        "        y = y.astype(np.float32).values\n",
        "\n",
        "    # Handle class imbalance through controlled oversampling\n",
        "    combined = pd.DataFrame(X)\n",
        "    combined['target'] = y\n",
        "\n",
        "    majority_class = combined[combined['target'] == 0]\n",
        "    minority_class = combined[combined['target'] == 1]\n",
        "\n",
        "    minority_oversampled = resample(\n",
        "        minority_class,\n",
        "        replace=True,\n",
        "        n_samples=int(len(minority_class) + oversample_ratio * len(majority_class)),\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    balanced_data = pd.concat([majority_class, minority_oversampled], axis=0)\n",
        "    balanced_data = balanced_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "    X = balanced_data.iloc[:, :-1].values\n",
        "    y = balanced_data['target'].values\n",
        "\n",
        "    return X, y"
      ],
      "metadata": {
        "id": "oKiGe3Mcj-M_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating a Simple Neural Network Model\n",
        "\n",
        "The `create_model` function defines a basic feedforward neural network:\n",
        "- 2 hidden layers with 64 and 32 neurons respectively.\n",
        "- `ReLU` activation for non-linearity.\n",
        "- A final layer with a sigmoid activation for binary classification.\n",
        "\n",
        "The model uses the Adam optimizer and binary cross-entropy loss.\n"
      ],
      "metadata": {
        "id": "gT14Z8enktiv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a simple neural network model\n",
        "def create_model(input_shape):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Input(shape=(input_shape,)),\n",
        "        tf.keras.layers.Dense(64, activation='relu'),\n",
        "        tf.keras.layers.Dense(32, activation='relu'),\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "zLQmM1t3kvfP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training a Model on a Single Task\n",
        "\n",
        "The `train_on_task` function:\n",
        "1. Trains the model on a given dataset for a specified number of epochs.We are training for 10 epochs.\n",
        "2. Records and returns the final training accuracy and training time.\n",
        "\n",
        "This function is essential for evaluating performance on individual tasks.\n"
      ],
      "metadata": {
        "id": "ufZIJR4ok9SS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to train a model on a single task\n",
        "def train_on_task(model, X_train, y_train, X_val, y_val, epochs=10):\n",
        "    start_time = time.time()\n",
        "    history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=epochs, verbose=0)\n",
        "    training_time = time.time() - start_time\n",
        "    return history.history['accuracy'][-1], training_time"
      ],
      "metadata": {
        "id": "206XH7ldlIy6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reptile Algorithm for Meta-Learning\n",
        "\n",
        "The `reptile` function:\n",
        "1. Initializes a meta-model to learn shared parameters across tasks.\n",
        "2. For each task:\n",
        "   - Trains a task-specific model.\n",
        "   - Updates the meta-model's parameters using the Reptile update rule.\n",
        "\n",
        "The meta-model learns an initialization that allows rapid adaptation to new tasks.\n"
      ],
      "metadata": {
        "id": "CNMvbrbclVzb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reptile algorithm implementation\n",
        "def reptile(train_data, num_tasks=4, epochs=10, num_inner_updates=5):\n",
        "    input_shape = train_data[0][0].shape[1]\n",
        "    meta_model = create_model(input_shape)\n",
        "\n",
        "    for task_idx in range(num_tasks):\n",
        "        print(f\"Training on task {task_idx + 1}/{num_tasks}\")\n",
        "        X, y = train_data[task_idx]\n",
        "        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "        task_model = create_model(input_shape)\n",
        "        for _ in range(num_inner_updates):\n",
        "            acc, training_time = train_on_task(task_model, X_train, y_train, X_val, y_val, epochs=epochs)\n",
        "            print(f\"Task {task_idx + 1}: Training accuracy: {acc:.4f}, Time: {training_time:.4f} seconds\")\n",
        "\n",
        "        weights = task_model.get_weights()\n",
        "        meta_model.set_weights([meta + (weight - meta) / (task_idx + 1) for meta, weight in zip(meta_model.get_weights(), weights)])\n",
        "\n",
        "    return meta_model\n"
      ],
      "metadata": {
        "id": "jPNIG_eglX0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluating the Model\n",
        "\n",
        "The `evaluate_model_with_report` function:\n",
        "1. Predicts labels for a test set using the trained model.\n",
        "2. Generates a classification report to evaluate precision, recall, and F1-score.\n",
        "\n",
        "This helps us in understanding the model's performance on  unseen data.\n"
      ],
      "metadata": {
        "id": "heazQvoQlbmL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to evaluate the model and print a classification report\n",
        "def evaluate_model_with_report(model, X_test, y_test):\n",
        "    y_pred = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
        "    print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "    loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "    return accuracy\n"
      ],
      "metadata": {
        "id": "OjdL0dMOlywU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the Meta-Model\n",
        "\n",
        "We train the meta-model on multiple datasets (`task_paths`). Each task contributes to learning a shared initialization through the Reptile algorithm.\n",
        "\n",
        "This step creates a general model capable of adapting to similar tasks.\n",
        "\n",
        "The datasets chosen have 32 features.We have tried to incorporate different types of financial fraud detection in our 4 training datasets to ensure our model adapts well across the domain. These datasets have been reduced to the desirable size from the original datasets using pca."
      ],
      "metadata": {
        "id": "stHlCDj_l3Bg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Paths to datasets\n",
        "task_paths = [\n",
        "    '/content/preprocessed_dataset1.csv',\n",
        "    '/content/reduced_dataset_32_features.csv',\n",
        "    '/content/reduced_dataset_creditcard_32_features.csv',\n",
        "    '/content/reduced_fraud_data_32_features.csv'\n",
        "]\n",
        "\n",
        "# Load datasets\n",
        "train_data = [load_data(path) for path in task_paths]\n",
        "\n",
        "# Train the meta-model\n",
        "meta_model = reptile(train_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yvmd-3Ekl2U8",
        "outputId": "d4561ccc-cb9a-4a87-a6ba-e6f486300df6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on task 1/4\n",
            "Task 1: Training accuracy: 0.9352, Time: 2.5351 seconds\n",
            "Task 1: Training accuracy: 0.9947, Time: 1.4642 seconds\n",
            "Task 1: Training accuracy: 1.0000, Time: 1.3511 seconds\n",
            "Task 1: Training accuracy: 1.0000, Time: 1.2753 seconds\n",
            "Task 1: Training accuracy: 1.0000, Time: 1.1904 seconds\n",
            "Training on task 2/4\n",
            "Task 2: Training accuracy: 0.9023, Time: 3.6717 seconds\n",
            "Task 2: Training accuracy: 0.9545, Time: 1.3379 seconds\n",
            "Task 2: Training accuracy: 0.9670, Time: 1.3146 seconds\n",
            "Task 2: Training accuracy: 0.9693, Time: 1.5100 seconds\n",
            "Task 2: Training accuracy: 0.9784, Time: 1.3939 seconds\n",
            "Training on task 3/4\n",
            "Task 3: Training accuracy: 0.9830, Time: 2.5110 seconds\n",
            "Task 3: Training accuracy: 0.9955, Time: 1.4073 seconds\n",
            "Task 3: Training accuracy: 0.9989, Time: 2.6711 seconds\n",
            "Task 3: Training accuracy: 0.9989, Time: 2.2762 seconds\n",
            "Task 3: Training accuracy: 1.0000, Time: 1.3557 seconds\n",
            "Training on task 4/4\n",
            "Task 4: Training accuracy: 0.8850, Time: 2.6091 seconds\n",
            "Task 4: Training accuracy: 0.9681, Time: 1.2969 seconds\n",
            "Task 4: Training accuracy: 0.9989, Time: 1.3054 seconds\n",
            "Task 4: Training accuracy: 1.0000, Time: 1.2895 seconds\n",
            "Task 4: Training accuracy: 1.0000, Time: 1.4512 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing the Meta-Model on a New Task\n",
        "\n",
        "We evaluate the meta-learned initialization by:\n",
        "1. Loading and preprocessing a new dataset (`new_task_path`).\n",
        "2. Training a model initialized with the meta-learned weights.\n",
        "3. Measuring its adaptability and performance on the new task.\n",
        "\n"
      ],
      "metadata": {
        "id": "P0s5X7bRosLl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the meta-learned model on a new task\n",
        "new_task_path = '/content/carclaims_2000.csv'\n",
        "X_new, y_new = load_data(new_task_path)\n",
        "X_new_train, X_new_val, y_new_train, y_new_val = train_test_split(X_new, y_new, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train on the new task\n",
        "\n",
        "new_task_model = create_model(X_new.shape[1])\n",
        "new_task_model.set_weights(meta_model.get_weights())  # Load weights from the meta-model\n",
        "start_time = time.time()\n",
        "\n",
        "train_on_task(new_task_model, X_new_train, y_new_train, X_new_val, y_new_val, epochs=20)\n",
        "training_time = time.time() - start_time\n",
        "\n",
        "# Evaluate and display results\n",
        "new_task_accuracy = evaluate_model_with_report(new_task_model, X_new_val, y_new_val)\n",
        "print(f\"New Task Model Training Time: {training_time:.4f} seconds\")\n",
        "print(f\"New Task Model Accuracy: {new_task_accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EYUd7dOxJ27_",
        "outputId": "192528cf-9751-4926-8e9b-4a056cab263a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.63      0.99      0.77       141\n",
            "         1.0       0.85      0.12      0.21        92\n",
            "\n",
            "    accuracy                           0.64       233\n",
            "   macro avg       0.74      0.55      0.49       233\n",
            "weighted avg       0.72      0.64      0.55       233\n",
            "\n",
            "New Task Model Training Time: 3.8752 seconds\n",
            "New Task Model Accuracy: 0.6438\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results and Conclusion\n",
        "\n",
        "- The meta-learned model demonstrates the ability to adapt quickly to new tasks.\n",
        "\n",
        "- This showcases the efficiency of the Reptile meta-learning approach for financial fraud detection.\n"
      ],
      "metadata": {
        "id": "vml2EWpnpMam"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Note:***\n",
        "The model's accuracy is not consistent it changes when we keep re-running the cell which is because of the high class imbalance present in the data.We are working towards solving that problem."
      ],
      "metadata": {
        "id": "2udRcsNoqec4"
      }
    }
  ]
}